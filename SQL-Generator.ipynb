{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"SQL-Generator.ipynb","provenance":[],"collapsed_sections":["i6rDDrAHFYR7","8npJRS-eFmc1","y3hSbp2DF4KM"],"mount_file_id":"1_4HayXLRlxhDgmxc4v_KBvLukaAkBEE2","authorship_tag":"ABX9TyMMmA3mopt+AiZSoZfktDSz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"cJ3ENZiUa86P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656186631041,"user_tz":-300,"elapsed":61470,"user":{"displayName":"Muhammad Ali","userId":"06846328763644788346"}},"outputId":"4b01ab00-5e25-4b4a-c8af-c875f2ec7a3b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tq_9cpJwamZ7","executionInfo":{"elapsed":6,"status":"ok","timestamp":1656186633229,"user":{"displayName":"Muhammad Ali","userId":"06846328763644788346"},"user_tz":-300},"outputId":"5763d302-e18b-458c-bc1d-17f026218b4b"},"source":["%cd \"/content/drive/MyDrive/Hackthons/gpt3-instruct-sandbox\""],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Hackthons/gpt3-instruct-sandbox\n"]}]},{"cell_type":"code","source":["!pip install openai\n","!pip install streamlit\n","!npm install -g localtunnel"],"metadata":{"id":"WKe1bZPnoCzP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import sys\n","import pandas as pd\n","from sqlalchemy import create_engine\n","import ipywidgets as widgets\n","from IPython.display import display\n","from IPython.display import update_display\n","from IPython.display import display_pretty\n","\n","from api import GPT, Example"],"metadata":{"id":"5dyc613_oC1z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Define display flow for GPT question input\n","def on_button_clicked(b):\n","    def sql_button_clicked(b):\n","        df = pd.read_sql(query, engine)\n","        df_pretty = df.style.hide_index().set_properties(**{'background-color': 'black',\n","                               'color' : 'lawngreen',\n","                               'border-color': 'white'})\n","        display(df_pretty)\n","        \n","    print ('\\033[1mInput:\\033[0m ' + inp.value)\n","    output = gpt.submit_request(inp.value)\n","    result = output['choices'][0].text\n","    query = result.split('output:')[1]\n","    print ('\\033[1mGPT-3 Response:\\033[0m ' + query)\n","    button2 = widgets.Button(description=\"Run SQL\")\n","    button2.on_click(sql_button_clicked)\n","    display(button2)"],"metadata":{"id":"2HJR3wwToC7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Construct GPT-3-instruct instance, add instruction and examples\n","gpt = GPT(engine=\"davinci-instruct-beta\",\n","          temperature=0.3,\n","          max_tokens=200)\n","gpt.add_instruction('Given an input question, respond with syntactically correct PostgreSQL.')\n","\n","gpt.add_example(Example('select columns from users table', \n","                        'select id, email, dt, plan_type, created_on, updated_on from users'))\n","gpt.add_example(Example('select columns from the charges table', \n","                        'select amount, amount_refunded, created, customer_id, status from charges'))\n","gpt.add_example(Example('select columns from the customers table', \n","                        'select created, email, id from customers'))\n","gpt.add_example(Example('how many users signed up in the past 30 days?', \n","                        \"SELECT COUNT(*) FROM users WHERE signup_dt >= now() - interval '30 days'\"))\n","gpt.add_example(Example('when did user with email brian@seekwell.io sign up?', \n","                        \"SELECT signup_dt FROM users WHERE email = 'brian@seekwell.io'\"))\n","gpt.add_example(Example('how much revenue did we have in the past 7 days?', \n","                        \"SELECT SUM(amount) from charges WHERE charge_dt >= now() - interval '7 days'\"))\n","gpt.add_example(Example('how many users signed up in the past 30 days?', \n","                        \"SELECT COUNT(*) FROM users WHERE signup_dt >= now() - interval '30 days'\"))\n","gpt.add_example(Example('how much revenue did we have from 10-01-20 through 11-15-20?', \n","                        \"SELECT SUM(case when charge_dt>= '10-01-20'::date and charge_dt < '11-15-20'::date then amount else 0 end) as revenue FROM charges\"))\n","gpt.add_example(Example('how much revenue have we had from users that signed up in the last 6 months?', \n","                        \"SELECT SUM(charges.amount) FROM users INNER JOIN charges ON users.id = charges.user_id WHERE users.signup_dt>= now() - interval '6 months'\"))\n","gpt.add_example(Example('when did user with email brian@seekwell.io make his first payment?', \n","                        \"SELECT MIN(charge_dt) as last_payment_dt from users INNER JOIN charges ON users.id = charges.user_id WHERE users.email = 'brian@seekwell.io'\"))\n","gpt.add_example(Example('how many new users signed up in each of the last 2 months?', \n","                        \"SELECT sum(case when signup_dt>= now() - interval '1 month' then 1 else 0 end) as signups_this_month, sum(case when signup_dt>= now() - interval '2 months' and signup_dt < now() - interval '1 month'  then 1 else 0 end) as signups_last_month FROM users\"))"],"metadata":{"id":"s1sgRzJDoC-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Display UI to give GPT-3 prompt and run and display resulting SQL\n","inp = widgets.Text(description='Ask GPT-3:') # input field with label\n","button1 = widgets.Button(description=\"Get GPT-3 Response\") # prediction button\n","#Box = widgets.HBox([inp, button1])\n","#print ('\\033[1mInstruction:\\033[0m ' + gpt.get_instruction_text())\n","#button1.on_click(on_button_clicked)\n","display(Box) # input field display"],"metadata":{"id":"MPnC6V09oDBK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile requirements.txt\n","openai\n","requests\n","streamlit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4gQZGEbGeXaC","executionInfo":{"status":"ok","timestamp":1656187794415,"user_tz":-300,"elapsed":297,"user":{"displayName":"Muhammad Ali","userId":"06846328763644788346"}},"outputId":"fdd1ea64-608a-4b4d-dd1a-a8b087087df4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing requirements.txt\n"]}]},{"cell_type":"code","source":["%%writefile streamlit_app.py\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sunday June 26 2022\n","@author: Muhammad Ali\n","@github: @alihussainia\n","\"\"\"\n","\n","import streamlit as st\n","from api import GPT, Example, set_openai_key\n","import os\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","st.set_page_config(page_title=\"SQL Generator\", layout='centered', initial_sidebar_state='auto', menu_items=None)\n","\n","st.title(\"SQL Generator Application\")\n","st.write(\"A web app that generates SQL queries using plain English\")\n","\n","inp = st.text_area(\"Please enter your query here\", max_chars=2000, height=150)\n","\n","# Get environment variables\n","key = os.getenv('key')\n","set_openai_key(key)\n","\n","#Construct GPT-3-instruct instance, add instruction and examples\n","gpt = GPT(engine=\"davinci-instruct-beta\",\n","          temperature=0.3,\n","          max_tokens=200)\n","gpt.add_instruction('Given an input question, respond with syntactically correct PostgreSQL.')\n","\n","gpt.add_example(Example('select columns from users table', \n","                        'select id, email, dt, plan_type, created_on, updated_on from users'))\n","gpt.add_example(Example('select columns from the charges table', \n","                        'select amount, amount_refunded, created, customer_id, status from charges'))\n","gpt.add_example(Example('select columns from the customers table', \n","                        'select created, email, id from customers'))\n","gpt.add_example(Example('how many users signed up in the past 30 days?', \n","                        \"SELECT COUNT(*) FROM users WHERE signup_dt >= now() - interval '30 days'\"))\n","gpt.add_example(Example('when did user with email brian@seekwell.io sign up?', \n","                        \"SELECT signup_dt FROM users WHERE email = 'brian@seekwell.io'\"))\n","gpt.add_example(Example('how much revenue did we have in the past 7 days?', \n","                        \"SELECT SUM(amount) from charges WHERE charge_dt >= now() - interval '7 days'\"))\n","gpt.add_example(Example('how many users signed up in the past 30 days?', \n","                        \"SELECT COUNT(*) FROM users WHERE signup_dt >= now() - interval '30 days'\"))\n","gpt.add_example(Example('how much revenue did we have from 10-01-20 through 11-15-20?', \n","                        \"SELECT SUM(case when charge_dt>= '10-01-20'::date and charge_dt < '11-15-20'::date then amount else 0 end) as revenue FROM charges\"))\n","gpt.add_example(Example('how much revenue have we had from users that signed up in the last 6 months?', \n","                        \"SELECT SUM(charges.amount) FROM users INNER JOIN charges ON users.id = charges.user_id WHERE users.signup_dt>= now() - interval '6 months'\"))\n","gpt.add_example(Example('when did user with email brian@seekwell.io make his first payment?', \n","                        \"SELECT MIN(charge_dt) as last_payment_dt from users INNER JOIN charges ON users.id = charges.user_id WHERE users.email = 'brian@seekwell.io'\"))\n","gpt.add_example(Example('how many new users signed up in each of the last 2 months?', \n","                        \"SELECT sum(case when signup_dt>= now() - interval '1 month' then 1 else 0 end) as signups_this_month, sum(case when signup_dt>= now() - interval '2 months' and signup_dt < now() - interval '1 month'  then 1 else 0 end) as signups_last_month FROM users\"))\n","\n","submit_button = st.button('Generate')\n","\n","if submit_button and inp==\"\":\n","  st.write(\"Please enter your problem above\")\n","\n","elif submit_button and inp!=\"\":\n","  output = gpt.submit_request(inp.value)\n","  result = output['choices'][0].text\n","  query = result.split('output:')[1]\n","  st.markdown(query) \n","\n","\n","st.text(\"App developed with ❤️ by @alihussainia\")\n","\n","st.text(f\"Connect with me via Email at malirashid1994@gmail.com\")"],"metadata":{"id":"Utr0djvluSzd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656187332065,"user_tz":-300,"elapsed":330,"user":{"displayName":"Muhammad Ali","userId":"06846328763644788346"}},"outputId":"645128c4-817f-465d-90cd-a265ef78d577"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing streamlit_app.py\n"]}]},{"cell_type":"code","source":["!streamlit run streamlit_app.py & npx localtunnel --port 8501"],"metadata":{"id":"1jP8OBFXc_mO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i6rDDrAHFYR7"},"source":["# Install Dependencies"]},{"cell_type":"code","metadata":{"id":"xbYM7XbYbcCl"},"source":["!apt install zstd\n","\n","# the \"slim\" version contain only bf16 weights and no optimizer parameters, which minimizes bandwidth and memory\n","!time wget -c https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd\n","\n","!time tar -I zstd -xf step_383500_slim.tar.zstd\n","\n","!git clone https://github.com/kingoflolz/mesh-transformer-jax.git\n","!pip install -r mesh-transformer-jax/requirements.txt\n","\n","# jax 0.2.12 is required due to a regression with xmap in 0.2.13\n","!pip install mesh-transformer-jax/ jax==0.2.12 tensorflow==2.5.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"76jAe8wMr3ix"},"source":["!git clone https://github.com/kingoflolz/mesh-transformer-jax.git\n","!pip install -r mesh-transformer-jax/requirements.txt\n","!pip install optax==0.0.9 transformers dm-haiku einops\n","!pip install ray"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aGvQYp2G3QIZ","executionInfo":{"elapsed":3781,"status":"ok","timestamp":1638829881716,"user":{"displayName":"Muhammad Ali","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06846328763644788346"},"user_tz":-300},"outputId":"0a71a39e-8df6-43e1-e6d6-c94ada341ec7"},"source":["!pip install mesh_transformer"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: mesh_transformer in /usr/local/lib/python3.7/dist-packages (0.0.0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"8npJRS-eFmc1"},"source":["# Setup Model"]},{"cell_type":"code","metadata":{"id":"tR2k66NrcYJD"},"source":["import os\n","import requests \n","from jax.config import config\n","\n","colab_tpu_addr = os.environ['COLAB_TPU_ADDR'].split(':')[0]\n","url = f'http://{colab_tpu_addr}:8475/requestversion/tpu_driver0.1_dev20210607'\n","requests.post(url)\n","\n","# The following is required to use TPU Driver as JAX's backend.\n","config.FLAGS.jax_xla_backend = \"tpu_driver\"\n","config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HJJlXdf4jODP"},"source":["import time\n","\n","import jax\n","from jax.experimental import maps\n","import numpy as np\n","import optax\n","import transformers\n","import mesh_transformer\n","import mesh_transformer.sampling\n","\n","from mesh_transformer.checkpoint import read_ckpt_lowmem\n","from mesh_transformer.sampling import nucleaus_sample\n","from mesh_transformer.transformer_shard import CausalTransformer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d843-gD5kUsQ"},"source":["params = {\n","  \"layers\": 28,\n","  \"d_model\": 4096,\n","  \"n_heads\": 16,\n","  \"n_vocab\": 50400,\n","  \"norm\": \"layernorm\",\n","  \"pe\": \"rotary\",\n","  \"pe_rotary_dims\": 64,\n","\n","  \"seq\": 2048,\n","  \"cores_per_replica\": 8,\n","  \"per_replica_batch\": 1,\n","}\n","\n","per_replica_batch = params[\"per_replica_batch\"]\n","cores_per_replica = params[\"cores_per_replica\"]\n","seq = params[\"seq\"]\n","\n","\n","params[\"sampler\"] = nucleaus_sample\n","\n","# here we \"remove\" the optimizer parameters from the model (as we don't need them for inference)\n","params[\"optimizer\"] = optax.scale(0)\n","\n","mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)\n","devices = np.array(jax.devices()).reshape(mesh_shape)\n","\n","params = {\n","  \"layers\": 28,\n","  \"d_model\": 4096,\n","  \"n_heads\": 16,\n","  \"n_vocab\": 50400,\n","  \"norm\": \"layernorm\",\n","  \"pe\": \"rotary\",\n","  \"pe_rotary_dims\": 64,\n","\n","  \"seq\": 2048,\n","  \"cores_per_replica\": 8,\n","  \"per_replica_batch\": 1,\n","}\n","\n","per_replica_batch = params[\"per_replica_batch\"]\n","cores_per_replica = params[\"cores_per_replica\"]\n","seq = params[\"seq\"]\n","\n","\n","params[\"sampler\"] = nucleaus_sample\n","\n","# here we \"remove\" the optimizer parameters from the model (as we don't need them for inference)\n","params[\"optimizer\"] = optax.scale(0)\n","\n","mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)\n","devices = np.array(jax.devices()).reshape(mesh_shape)\n","\n","maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp')))\n","\n","tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')\n","tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bs_qRVXkFuqf"},"source":["Here we create the network and load the parameters from the downloaded files."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bvj6ctSu4XHr","executionInfo":{"status":"ok","timestamp":1638833672132,"user_tz":-300,"elapsed":313457,"user":{"displayName":"Muhammad Ali","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06846328763644788346"}},"outputId":"8f37d3fa-66f2-4c4b-ed42-d7922bef9ce9"},"source":["total_batch = per_replica_batch * jax.device_count() // cores_per_replica\n","\n","network = CausalTransformer(params)\n","\n","network.state = read_ckpt_lowmem(network.state, \"step_383500/\", devices.shape[1])\n","\n","network.state = network.move_xmap(network.state, np.zeros(cores_per_replica))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/jax/experimental/maps.py:412: UserWarning: xmap is an experimental feature and probably has bugs!\n","  warn(\"xmap is an experimental feature and probably has bugs!\")\n"]},{"output_type":"stream","name":"stdout","text":["key shape (8, 2)\n","in shape (1, 2048)\n","dp 1\n","mp 8\n","Total parameters: 6053381344\n","read from disk/gcs in 189.236s\n"]}]},{"cell_type":"markdown","metadata":{"id":"y3hSbp2DF4KM"},"source":["# Run Model\n","\n","Finally, we are ready to infer with the model! The first sample takes around a minute due to compilation, but after that it should only take about 10 seconds per sample.\n","\n","Feel free to mess with the different sampling parameters (top_p and temp), as well as the length of the generations (gen_len, causes a recompile when changed).\n","\n","You can also change other things like per_replica_batch in the previous cells to change how many generations are done in parallel. A larger batch has higher latency but higher throughput when measured in tokens generated/s. This is useful for doing things like best-of-n cherry picking.\n","\n","*Tip for best results: Make sure your prompt does not have any trailing spaces, which tend to confuse the model due to the BPE tokenization used during training.*"]},{"cell_type":"code","metadata":{"id":"Az25BZ0C4e90"},"source":["# allow text wrapping in generated output: https://stackoverflow.com/a/61401455\n","from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":503},"id":"uW2VOl6f6BVl","executionInfo":{"status":"ok","timestamp":1638833733711,"user_tz":-300,"elapsed":61589,"user":{"displayName":"Muhammad Ali","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06846328763644788346"}},"outputId":"50c4b380-35ed-4de5-e934-60c1f53cab4e"},"source":["def infer(context, top_p=0.9, temp=1.0, gen_len=512):\n","    tokens = tokenizer.encode(context)\n","\n","    provided_ctx = len(tokens)\n","    pad_amount = seq - provided_ctx\n","\n","    padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)\n","    batched_tokens = np.array([padded_tokens] * total_batch)\n","    length = np.ones(total_batch, dtype=np.uint32) * len(tokens)\n","\n","    start = time.time()\n","    output = network.generate(batched_tokens, length, gen_len, {\"top_p\": np.ones(total_batch) * top_p, \"temp\": np.ones(total_batch) * temp})\n","\n","    samples = []\n","    decoded_tokens = output[1][0]\n","\n","    for o in decoded_tokens[:, :, 0]:\n","      samples.append(f\"\\033[1m{context}\\033[0m{tokenizer.decode(o)}\")\n","\n","    print(f\"completion done in {time.time() - start:06}s\")\n","    return samples\n","\n","print(infer(\"EleutherAI is\")[0])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["completion done in 61.43248200416565s\n","\u001b[1mEleutherAI is\u001b[0m a bot that runs within chat, able to read text, list, and randomise text, but with a limited number of responses. It is able to parse text and generate a message based on an input string. You can use it in various ways, but it can be best used for converting a user's input into formatted text, especially when you want a certain amount of responses to be used up.\n","\n","Originally started for a user-made site called Megagun, it is developed to be fun, and is not yet actually useful as a tool.\n","\n","In both Irc and Skype you can take advantage of the fact that eThereAI is allowed to give customisable, and even random responses. It works well with people who do not always want an answer to their question, as it can also generate canned responses such as \"Tell me more\", \"What would you like to know?\", \"Just put your question in the box.\" and many more. In any case, an improvement over most existing bots would be helpful for most of the community.\n","\n","Step 1: Creating the Generator\n","\n","The first thing to do is to create a generator file, which will tell eThereAI how to generate and use your responses. Let us call it Response.esk, we need to place all of our response in a text file, but we can use a program called Syntax Highlighter or Markdown Editor to make the job easier. If the file is too long, just place each response in a new line, and all responses will be ordered according to their order in the file.\n","\n","Line #1: \"if [[ -n $(date +%s) ]] then return true\\n\"; This is just a conditional tag to place our responses in order.\n","\n","Line #2: \"if [[ -n $(date +%s) ]] then return true\\n\"; This is just a conditional tag to place our responses in order.\n","\n","Line #3: \"if [[ -n $(date +%s) ]] then return true\\n\"; This is just a conditional tag to place our responses in order.\n","\n","Line #4: ; We need to inform the eThereAI what to do if there are no responses in the file.\n","\n","Line #4: ; As we are looking for responses that start with either \"if\" or \"then\", \":\" or \"fi\" are allowed in a response.\n","\n","Line #4: [If [\"$(\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":503},"id":"OMud_WVU6DJq","executionInfo":{"status":"ok","timestamp":1638833790204,"user_tz":-300,"elapsed":13921,"user":{"displayName":"Muhammad Ali","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06846328763644788346"}},"outputId":"d365f129-116d-4a37-b5cb-c4a4673f5eed"},"source":["#@title  { form-width: \"300px\" }\n","top_p = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","temp = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","\n","context = \"\"\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\"\"\n","\n","print(infer(top_p=top_p, temp=temp, gen_len=512, context=context)[0])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["completion done in 13.55186653137207s\n","\u001b[1mIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\u001b[0m\n","\n","This is no typical New World Camelopardalis, but one of three subspecies of the South American camelopardis, a camel-like, ungulate native to the Andes and Odriozé biomes of Bolivia, Peru, Ecuador and Colombia. A spectacular wild horse of the genus is protected under law, and there are fewer than 1,500 in the wild, which is far too few for the species to have gone extinct.\n","\n","The Andean unicorn is an endangered species of the Pecari, or peccary, family. These wild horses have a thick neck, broad skull, long, lanky legs and a dainty tuft of hair on their backs and tails. “It looks really kind of cute,” as Donald Trump would say, but I never understood why, out of all the different varieties of native ungulates, they always were named “unicorn.”\n","\n","According to the Wildlife Conservation Society, the South American camelopardis or peccary is one of the most evolutionarily distinct wild horse species in the world. Besides the Andean unicorn, there are two other subspecies: the northern Andean peccary (C. atlanticus) and the short-horned peccary (C. orthoptilus). The scientists are also investigating a fourth subspecies: the Bolivian horse or wild Andean pony (C. boliviensis).\n","\n","The species is extremely rare. Scientists discovered it in 2003 when they found an unrecorded subspecies of peccary. Using different genetic markers and DNA-based genealogy, they have identified all three extant wild subspecies and their limited distribution. Most of the research is being conducted by the Caviunca Center for Conservation at Los Andes University, the GSI of Brazil and the National Geographic Society.\n","\n","The ‘unicorn’ lives in the Argentine Andes.\n","\n","“The wild peccaries are most similar in morphology to the domestic horse, but a lot of characteristics are not the same,” said Dr. Roberto Boscana, Executive Director of the GSI, a genetic biodiversity center in Brazil.\n","\n","In 2006, Boscana and his colleagues—whose main tool was DNA sampling—discovered the horses, and they brought back a group of domestic horses to breed.\n","\n","“This is one of the oldest experiments, because the horses came from Santa Cruz de la Sierra,”\n"]}]}]}